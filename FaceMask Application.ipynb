{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66079c8b-2d77-44d0-ba8e-8e432261bb3f",
   "metadata": {},
   "source": [
    "### Import relevent Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82df0bd6-6dce-48da-99ad-c912419e820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46db1886-fafb-4646-ab60-5d8b61976e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
    "\n",
    "    # Photoframe\n",
    "    (h,w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0,(300,300),   # Converts the input frame (frame) into a \"blob\" to make it compatible with the neural network.\n",
    "                                (104.0,177.0,123.0))\n",
    "\n",
    "    # Capture / Crop -> Face & Location \n",
    "    # Sets the preprocessed image (blob) as input to the face detection network (faceNet)\n",
    "    faceNet.setInput(blob)         # blob -> Input Image \n",
    "    detections = faceNet.forward()\n",
    "\n",
    "    faces = []    # List of face images cropped from the frame\n",
    "    locs = []      # List of face locations (bounding boxes)\n",
    "    preds = []    # List of predictions from the facemask network\n",
    "\n",
    "    # Loop over the detections\n",
    "    detections.shape\n",
    "    \n",
    "    for i in range(0, detections.shape[2]):    # To detect how many faces in the frame\n",
    "        confidence = detections[0,0,i,2]\n",
    "\n",
    "    # If the face is captured from the frame\n",
    "    \n",
    "    if len(faces) > 0:\n",
    "        faces = np.array(faces, dtype='float32')   # Convert matrix -> array \n",
    "        pred = maskNet.predict(faces, batch_size=32)    # maskNet is our trained model using which we are going to predict\n",
    "        \n",
    "        # For faster inference we are using batch_size predictions on all faces at a time rather one-by-one predictions in the above For Loop\n",
    "\n",
    "    return(locs, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a700c-941e-4a31-94b3-0458eafb990a",
   "metadata": {},
   "source": [
    "### Construct the argument parser and parse the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c112f84-c79a-414c-885e-49db2be81f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-f\", \"--face\", type=str,\n",
    "    default=\"face_detector\",\n",
    "    help=\"path to face detector model directory\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str,\n",
    "    default=\"face_mask_model.h5\",\n",
    "    help=\"path to trained face mask detector model\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "    help=\"minimum probability to filter weak detections\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4521b6-cdf1-4182-acb0-f99c947a93b5",
   "metadata": {},
   "source": [
    "### Load Pre-Trained Model - from OpenCV (cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7449d9bd-2664-43b1-b736-16320a8f3e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face directory path: C:\\Users\\priya\\AppData\\Roaming\\jupyter\\runtime\\kernel-0d951486-5874-4fee-895d-e988eeacd07a.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Face directory path:\", args['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88034955-0ac2-4c4a-9dac-b06876874682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read-only attribute removed from the folder.\n",
      "Successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from keras.models import load_model\n",
    "\n",
    "# Step 1: Define the folder path\n",
    "folder_path = r'C:\\Priya learning path\\Hope_Artifiacial_Intelligence\\Deep Learning\\Face Mask Detection Project\\face_detector'\n",
    "\n",
    "# Step 2: Remove the read-only attribute\n",
    "try:\n",
    "    subprocess.run(f'attrib -r \"{folder_path}\" /s /d', shell=True)\n",
    "    print(\"Read-only attribute removed from the folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to change folder properties: {e}\")\n",
    "\n",
    "# Step 3: Save the model\n",
    "try:\n",
    "    prototxtPath = os.path.join(folder_path,'deploy.prototxt')\n",
    "    weightsPath = os.path.join(folder_path, 'res10_300x300_ssd_iter_140000.caffemodel')\n",
    "    faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "    print(\"Successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save the model: {e}\")\n",
    "    \n",
    "# args['face'] -> Refers to the directory path containing the model files (deploy.prototxt and res10_300x300_ssd_iter_140000.caffemodel).\n",
    "# prototxtPath: The path to the .prototxt file, which defines the architecture of the deep learning model (layers, connections, etc.).\n",
    "# weightsPath: The path to the .caffemodel file contains the pre-trained weights for the model.\n",
    "# cv2.dnn.readNet() returns a Net object representing the loaded model. This object can then be used for inference (e.g., detecting faces in an image).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fbff34-0760-4111-ac98-4ff31451859b",
   "metadata": {},
   "source": [
    "### Load Our Facemask detector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b16e5f6a-4979-44de-beb1-f1c69ca48b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskNet = load_model(args['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a9c6945-72dd-4afd-8690-65be2ba47ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It supports video sources such as webcams, USB cameras, or video files\n",
    "\n",
    "vs = VideoStream(src=0).start()   # 0 refers to the default webcam on your system.\n",
    "time.sleep(2.0)                   # Introduces a delay of 2 seconds to give the camera hardware time to \"warm up.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e6aa13-a825-4208-a028-92a1c7bb4547",
   "metadata": {},
   "source": [
    "### Loop over the frames from the video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da95aad-5b03-46bd-9943-b37682333ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # grab the frame from the threaded video stream and resize it to have a maximum width of 400 pixels\n",
    "    frame = vs.read() \n",
    "    frame = imutils.resize(frame, width=400)   # frame -> becomes a single image (in numpy array format) \n",
    "                                               # imutils -> which simplifies resizing operations\n",
    "    \n",
    "    # Detect faces in the frame and determine if they are wearing a face mask or not\n",
    "    locs, preds = detect_and_predict_mask(frame, faceNet, maskNet)\n",
    "\n",
    "    # Loop over the detected face locations and their corresponding locations\n",
    "    for box, preds in zip(locs, preds):\n",
    "        startX, startY, endX, endY = box     #unpack the bounding box and predictions\n",
    "        with_mask, without_mask = pred\n",
    "\n",
    "        # Determine the class label, box color & text color\n",
    "        label = 'Mask' if with_mask > without_mask else 'NO Mask'\n",
    "        color = (0,255,0) if label == 'Mask' else (0,0,255)    # OpenCV uses BGR (Blue, Green, Red) format instead of RGB\n",
    "    \n",
    "        # Display the label and bounding box on the output frame  \n",
    "        if label == 'Mask':\n",
    "            cv2.putText(frame, \"Mask : You are allowed\", (startX, startY - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2) \n",
    "        elif label == 'NO Mask':\n",
    "            cv2.putText(frame, \"No Mask : You are NOT allowed\", (startX, startY - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2) \n",
    "\n",
    "    # Show the Output Frame \n",
    "    cv2.imshow('Frame', frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # If the `q` key was pressed, break from the loop\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cv2. destroyAllwindows()\n",
    "\n",
    "vs.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2f130-0d26-4d91-951e-afdb101783be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d45d97-5f7c-4de4-8da0-abfa9fb6d6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef5ae8-5b07-4c8e-bfe3-1aaa688220f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a765639-6269-4299-8d53-1b74fe9ff9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bdd0c5-6828-4c3e-bba7-897c4dfc287d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302c37b-483c-4f31-b6f6-156a1fec2065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc089de9-4c11-4baf-b8b7-4543a3a1f3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb90fc-7009-4ad5-b968-41f335dfdaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
